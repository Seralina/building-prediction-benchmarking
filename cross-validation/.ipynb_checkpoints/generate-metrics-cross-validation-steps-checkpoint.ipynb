{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import TimeSeriesSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = pd.read_csv(\"../input/meta_open.csv\", index_col='uid', parse_dates=[\"datastart\",\"dataend\"], dayfirst=True)\n",
    "temporal = pd.read_csv(\"../input/temp_open_utc_complete.csv\", index_col='timestamp', parse_dates=True).tz_localize('utc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "buildingnames = temporal.columns[temporal.columns.str.contains(\"Office\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kitkat\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "# Import all models we are using\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.tree import ExtraTreeRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from  sklearn.linear_model import HuberRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import PassiveAggressiveRegressor\n",
    "from sklearn.linear_model import RANSACRegressor\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.linear_model import TheilSenRegressor\n",
    "    \n",
    "# Make array of models. Each model is an array of two elements.\n",
    "# First element is a model-name, second is a model itself\n",
    "models = [#['RandomForestRegressor', RandomForestRegressor(n_estimators = 1000, random_state = 42)],\n",
    "#['AdaBoostRegressor', AdaBoostRegressor(n_estimators = 1000, random_state = 42)],\n",
    "#['BaggingRegressor', BaggingRegressor(n_estimators = 1000, random_state = 42)],\n",
    "#['DecisionTreeRegressor', DecisionTreeRegressor(random_state = 42)],\n",
    "['DummyRegressor', DummyRegressor()],\n",
    "#['ExtraTreeRegressor', ExtraTreeRegressor(random_state = 42)],\n",
    "#['ExtraTreesRegressor', ExtraTreesRegressor(n_estimators = 1000, random_state = 42)],\n",
    "#['GaussianProcessRegressor', GaussianProcessRegressor(random_state = 42)],\n",
    "#['GradientBoostingRegressor', GradientBoostingRegressor(n_estimators = 1000, random_state = 42)],\n",
    "#['HuberRegressor', HuberRegressor()],\n",
    "#['KNeighborsRegressor', KNeighborsRegressor()],\n",
    "#['MLPRegressor', MLPRegressor(random_state = 42)],\n",
    "#['PassiveAggressiveRegressor', PassiveAggressiveRegressor(random_state = 42)],\n",
    "#['RANSACRegressor', RANSACRegressor(random_state = 42)],\n",
    "#['SGDRegressor', SGDRegressor(random_state = 42)],\n",
    "#['TheilSenRegressor', TheilSenRegressor(random_state = 42)]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce file with metrics(MAPE, NMBE, CVRSME, RSQUARED) based on provided model\n",
    "# Results will be saved as modelName_metrics.csv\n",
    "def createMetrics(modelName, model):\n",
    "    print('\\n\\n' + modelName + '\\n_____________')\n",
    "    # buidingindex\n",
    "    buildingindex = 0\n",
    "    for singlebuilding in buildingnames[:3]:\n",
    "        buildingindex+=1\n",
    "        print(\"Modelling: \" + singlebuilding)\n",
    "        # Get Data\n",
    "        single_timezone = meta.T[singlebuilding].timezone\n",
    "        single_start = meta.T[singlebuilding].datastart\n",
    "        single_end = meta.T[singlebuilding].dataend\n",
    "        single_building_data = pd.DataFrame(temporal[singlebuilding].tz_convert(single_timezone).truncate(before=single_start,after=single_end))\n",
    "            \n",
    "        # split time series data samples\n",
    "        # Create an array that's in the same order as the specific building loaded\n",
    "        months = np.array([single_building_data.index.month.unique()])[0]\n",
    "        n_splits = 3\n",
    "        tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "        \n",
    "        # Get weather file\n",
    "        weatherfilename = meta.T[singlebuilding].newweatherfilename\n",
    "        #print(\"Weatherfile: \"+weatherfilename)\n",
    "        weather = pd.read_csv(os.path.join(\"../input/\",weatherfilename),index_col='timestamp', parse_dates=True, na_values='-9999')\n",
    "        weather = weather.tz_localize(single_timezone, ambiguous = 'infer')\n",
    "        outdoor_temp = pd.DataFrame(weather[[col for col in weather.columns if 'Temperature' in col]]).resample(\"H\").mean()\n",
    "        outdoor_temp = outdoor_temp.reindex(pd.DatetimeIndex(start=outdoor_temp.index[0], periods=len(single_building_data), freq=\"H\")).fillna(method='ffill').fillna(method='bfill')\n",
    "        \n",
    "        # Cross validation step\n",
    "        index = 0\n",
    "        for train_index, test_index in tscv.split(months):  \n",
    "            month_train, month_test = months[train_index], months[test_index]\n",
    "            # Split into Training and Testing\n",
    "            trainingdata = single_building_data[single_building_data.index.month.isin(month_train)]\n",
    "            testdata = single_building_data[single_building_data.index.month.isin(month_test)]\n",
    "           \n",
    "            # Create training data array\n",
    "            train_features = np.array(pd.concat([pd.get_dummies(trainingdata.index.hour),\n",
    "                                                 pd.get_dummies(trainingdata.index.dayofweek),\n",
    "                       pd.Series(outdoor_temp[outdoor_temp.index.month.isin(month_train)].TemperatureC.values)], axis=1))\n",
    "            train_labels = np.array(trainingdata[singlebuilding].values)\n",
    "\n",
    "            # Create test data array\n",
    "            test_features = np.array(pd.concat([pd.get_dummies(testdata.index.hour),\n",
    "                                                 pd.get_dummies(testdata.index.dayofweek),\n",
    "                       pd.Series(outdoor_temp[outdoor_temp.index.month.isin(month_test)].TemperatureC.values)], axis=1))\n",
    "            test_labels = np.array(testdata[singlebuilding].values)\n",
    "\n",
    "\n",
    "            # Train the model on training data\n",
    "            model.fit(train_features, train_labels);\n",
    "            # Use the forest's predict method on the test data\n",
    "            predictions = model.predict(test_features)\n",
    "\n",
    "            # Calculate the absolute errors\n",
    "            errors = abs(predictions - test_labels)\n",
    "            # Calculate mean absolute percentage error (MAPE) and add to list\n",
    "            MAPE = 100 * np.mean((errors / test_labels))\n",
    "            NMBE = 100 * (sum(test_labels - predictions) / (pd.Series(test_labels).count() * np.mean(test_labels)))\n",
    "            CVRSME = 100 * ((sum((test_labels - predictions)**2) / (pd.Series(test_labels).count()-1))**(0.5)) / np.mean(test_labels)\n",
    "            RSQUARED = r2_score(test_labels, predictions)\n",
    "\n",
    "            #print(\"MAPE: \"+str(MAPE))\n",
    "            #print(\"NMBE: \"+str(NMBE))\n",
    "            #print(\"CVRSME: \"+str(CVRSME))\n",
    "            #print(\"R SQUARED: \"+str(RSQUARED))\n",
    "\n",
    "            #MAPE_data[singlebuilding] = MAPE\n",
    "            #NMBE_data[singlebuilding] = NMBE\n",
    "            #CVRSME_data[singlebuilding] = CVRSME\n",
    "            #RSQUARED_data[singlebuilding] = RSQUARED\n",
    "            \n",
    "            index+=1\n",
    "            if(buildingindex == 1):\n",
    "                temporary = pd.DataFrame(columns=[\"building\", \"MAPE\", \"NMBE\", \"CVRSME\", \"RSQUARED\"])\n",
    "                temporary.to_csv('./results-timeseries/' + modelName + '_metrics_cross_validation_' + str(index) + '.csv', index=False)\n",
    "            # Read dataframe with particular step (cross validation) \n",
    "            metrics_prev = pd.read_csv('./results-timeseries/' + modelName + '_metrics_cross_validation_' + str(index) + '.csv')\n",
    "            df = pd.DataFrame([[singlebuilding, MAPE, NMBE, CVRSME, RSQUARED]],columns=['building','MAPE','NMBE','CVRSME','RSQUARED'])\n",
    "            # Append new row\n",
    "            metrics = pd.concat([df,metrics_prev])\n",
    "            metrics.to_csv('./results-timeseries/' + modelName + '_metrics_cross_validation_' + str(index) + '.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "DummyRegressor\n",
      "_____________\n",
      "Modelling: Office_Cristina\n",
      "Modelling: Office_Jesus\n",
      "Modelling: Office_Jett\n"
     ]
    }
   ],
   "source": [
    "MAPE_data = {}\n",
    "RSQUARED_data = {}\n",
    "NMBE_data = {}\n",
    "CVRSME_data = {}\n",
    "for elem in models:\n",
    "    # modelName = elem[0], model = elem[1]\n",
    "    createMetrics(elem[0], elem[1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.read_csv(\"../cross-validation/results-timeseries/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
